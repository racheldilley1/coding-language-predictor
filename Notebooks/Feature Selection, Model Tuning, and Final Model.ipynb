{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from scipy import stats\n",
    "from collections import defaultdict\n",
    "\n",
    "#modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import ADASYN, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from classification_functions import  conf_matrix, plot_roc\n",
    "from sklearn.metrics import f1_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGBoost(X_train, y_train, X_val, y_val, depth, l_rate, subsample,  min_weight, col_sample):\n",
    "\n",
    "    params = { \n",
    "                'n_estimators': 20000,\n",
    "                'max_depth': depth,\n",
    "                'objective':'multi:softprob',\n",
    "                'num_classes' :3,  \n",
    "                'learning_rate': l_rate, \n",
    "                'subsample': subsample,\n",
    "                'min_child_weight':min_weight,\n",
    "                'colsample_bytree':col_sample,\n",
    "                'random_state' : 0,\n",
    "                'verbosity' : 0,\n",
    "                'n_jobs' : -1}\n",
    "\n",
    "    gbm = XGBClassifier()\n",
    "    gbm.set_params(**params)\n",
    "    gbm.fit(X_train, y_train)\n",
    "    preds = gbm.predict(X_val)\n",
    "    \n",
    "    print(f'XGBoost with params:\\n'\n",
    "          f'max_depth = {depth}\\n'\n",
    "          f'learning_rate = {l_rate}\\n'\n",
    "          f'subsample = {subsample}\\n'\n",
    "          f'min_child_weight = {min_weight}\\n'\n",
    "          f'colsample_bytree = {col_sample}\\n'\n",
    "          '\\n'\n",
    "          f'Has an f1 score of: {round(f1_score( y_val, preds, average=\"macro\"), 3)}'\n",
    "         )\n",
    "          \n",
    "    return gbm\n",
    "\n",
    "def XGBoost_baseline(X_train, y_train):\n",
    "              \n",
    "    gbm = XGBClassifier()\n",
    "    gbm.fit(X_train, y_train)\n",
    "    \n",
    "          \n",
    "    return gbm\n",
    "\n",
    "def get_f1(model, X_val, y_val):\n",
    "    preds = model.predict(X_val)\n",
    "    \n",
    "    return f1_score( y_val, preds, average=\"macro\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../Data/survey_data_cleaned2.pkl')\n",
    "#df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['database_count', 'Age1stCode', 'YearsCodePro', 'Age', 'YearsCode',\n",
      "       'EdLevel_BA/BS', 'EdLevel_MA/PhD', 'EdLevel_None', 'EdLevel_Prof',\n",
      "       'EdLevel_Some Univ', 'EdLevel_Student', 'Gender_Woman',\n",
      "       'Gender_gender non-conforming', 'UndergradMajor_Comp Sci/Eng',\n",
      "       'UndergradMajor_Eng', 'UndergradMajor_Health/Nat Sci',\n",
      "       'UndergradMajor_Human/Social Sci', 'UndergradMajor_Info Tech/Sys',\n",
      "       'UndergradMajor_Math/Stats', 'UndergradMajor_None',\n",
      "       'UndergradMajor_Web Dev', 'Region_Asia', 'Region_Australia',\n",
      "       'Region_Baltics', 'Region_CIS', 'Region_Europe', 'Region_M East',\n",
      "       'Region_N America', 'Region_S America', 'Region_other', 'Hobbyist_Yes',\n",
      "       'back-end_Yes', 'full-stack_Yes', 'front-end_Yes', 'desktop_Yes',\n",
      "       'mobile_Yes', 'DevOps_Yes', 'Database admin_Yes', 'Designer_Yes',\n",
      "       'System admin_Yes', 'Student_Yes', 'Other Occupation_Yes',\n",
      "       'Retired Dev_Yes', 'Sometimes Code at Work_Yes', 'JavaScript_Yes',\n",
      "       'Python_Yes', 'SQL_Yes', 'Java_Yes', 'HTML/CSS_Yes'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "X = df[['database_count', 'Age1stCode', 'YearsCodePro', 'Age','YearsCode', 'EdLevel', \n",
    "         'Gender', 'UndergradMajor', 'Region', \n",
    "        'Hobbyist', 'back-end', 'full-stack', 'front-end', 'desktop', 'mobile', 'DevOps', 'Database admin', \n",
    "        'Designer','System admin', 'Student', 'Other Occupation', 'Retired Dev','Sometimes Code at Work', \n",
    "       'JavaScript', 'Python', 'SQL', 'Java', 'HTML/CSS']]\n",
    "y = df['OpSys']\n",
    "\n",
    "X = pd.get_dummies(X, drop_first = True)\n",
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try ADASYN Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/racheldilley/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  The variable **evals_result** will contain:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost with params:\n",
      "max_depth = 10\n",
      "learning_rate = 0.3\n",
      "subsample = 0.8\n",
      "min_child_weight = 3\n",
      "colsample_bytree = 0.8\n",
      "\n",
      "Has an f1 score of: 0.848\n"
     ]
    }
   ],
   "source": [
    "X_adasyn, y_adasyn = ADASYN(random_state=42).fit_sample(X,y)\n",
    "\n",
    "X_train_adasyn, X_test_adasyn, y_train_adasyn, y_test_adasyn = train_test_split(X_adasyn,y_adasyn, test_size=0.2, random_state=42)\n",
    "X_train_adasyn, X_val_adasyn, y_train_adasyn, y_val_adasyn = train_test_split(X_train_adasyn,y_train_adasyn, test_size=0.25, random_state=42)\n",
    "\n",
    "xgb_adasyn = XGBoost(X_train_adasyn, y_train_adasyn,X_val_adasyn, y_val_adasyn, 10, 0.3, 0.8, 3, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try SMOTE Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/racheldilley/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  The variable **evals_result** will contain:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost with params:\n",
      "max_depth = 10\n",
      "learning_rate = 0.3\n",
      "subsample = 0.8\n",
      "min_child_weight = 3\n",
      "colsample_bytree = 0.8\n",
      "\n",
      "Has an f1 score of: 0.849\n"
     ]
    }
   ],
   "source": [
    "X_smoted, y_smoted = SMOTE(random_state=42).fit_sample(X,y)\n",
    "\n",
    "X_train_smoted, X_test_smoted, y_train_smoted, y_test_smoted = train_test_split(X_smoted,y_smoted, test_size=0.2, random_state=42)\n",
    "X_train_smoted, X_val_smoted, y_train_smoted, y_val_smoted = train_test_split(X_train_smoted, y_train_smoted, test_size=0.25, random_state=42)\n",
    "\n",
    "xgb_smoted = XGBoost(X_train_smoted, y_train_smoted, X_val_smoted, y_val_smoted, 10, 0.3, 0.8, 3, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/racheldilley/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  The variable **evals_result** will contain:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost with params:\n",
      "max_depth = 10\n",
      "learning_rate = 0.3\n",
      "subsample = 0.8\n",
      "min_child_weight = 3\n",
      "colsample_bytree = 0.8\n",
      "\n",
      "Has an f1 score of: 0.742\n"
     ]
    }
   ],
   "source": [
    "X_under, y_under = RandomUnderSampler(random_state=42).fit_sample(X,y)\n",
    "X_train_under, X_test_under, y_train_under, y_test_under = train_test_split(X_under,y_under, test_size=0.2, random_state=42)\n",
    "X_train_under, X_val_under, y_train_under, y_val_under = train_test_split(X_train_under,y_train_under, test_size=0.25, random_state=42)\n",
    "\n",
    "xgb_under = XGBoost(X_train_under, y_train_under, X_val_under, y_val_under, 10, 0.3, 0.8, 3, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE performed the best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pickle balanced dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.concat((X_smoted,y_smoted), axis=1)\n",
    "# df.to_pickle('../Data/survey_data_cleaned_balanced.pkl')\n",
    "# #print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from itertools import islice\n",
    "def remove_features(X_train, y_train, X_val, y_val, basef1):\n",
    "    feats_remove = []\n",
    "    keep_remove = 1\n",
    "    col_list = list(X_train.columns)\n",
    "    #print(col_list)\n",
    "    d = {el:0 for el in col_list}\n",
    "    \n",
    "    while keep_remove == 1:\n",
    "        idx = 0\n",
    "        for c in col_list:\n",
    "            removed = col_list[idx]\n",
    "            #print(removed)\n",
    "            \n",
    "            removed_col_list = col_list.copy()\n",
    "            removed_col_list.remove(removed)\n",
    "            \n",
    "            #print(removed_col_list)\n",
    "            model = XGBoost_baseline(X_train[removed_col_list], y_train)\n",
    "            \n",
    "            d[removed] = get_f1(model, X_val[removed_col_list], y_val)\n",
    "            idx = idx + 1\n",
    "        \n",
    "        max_f1_key = max(d, key=d.get)\n",
    "        max_f1_val = d[max_f1_key]\n",
    "        print(max_f1_key, max_f1_val)\n",
    "        \n",
    "        if max_f1_val >= basef1:\n",
    "            col_list.remove(max_f1_key)\n",
    "            feats_remove.append(max_f1_key)\n",
    "            print(feats_remove)\n",
    "            d = {el:0 for el in col_list}\n",
    "        else:\n",
    "            keep_remove = 0\n",
    "    \n",
    "    return feats_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6268231035372128\n"
     ]
    }
   ],
   "source": [
    "xgb_smoted_base = XGBoost_baseline(X_train_smoted, y_train_smoted)\n",
    "basef1 = get_f1(xgb_smoted_base, X_val_smoted, y_val_smoted)\n",
    "print(basef1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hobbyist_Yes 0.6313012755739176\n",
      "Other Occupation_Yes 0.6312945532891546\n",
      "Designer_Yes 0.631522974787115\n",
      "Sometimes Code at Work_Yes 0.6300528146284038\n",
      "Region_CIS 0.6313074196277163\n",
      "Retired Dev_Yes 0.6309844026629043\n",
      "Region_Baltics 0.6324255427584998\n",
      "full-stack_Yes 0.6300788991152816\n",
      "UndergradMajor_Human/Social Sci 0.6284730799021538\n",
      "EdLevel_None 0.6300873273579417\n",
      "JavaScript_Yes 0.6302452645554295\n",
      "Gender_gender non-conforming 0.6300093257551286\n",
      "UndergradMajor_None 0.6267852968008631\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hobbyist_Yes',\n",
       " 'Other Occupation_Yes',\n",
       " 'Designer_Yes',\n",
       " 'Sometimes Code at Work_Yes',\n",
       " 'Region_CIS',\n",
       " 'Retired Dev_Yes',\n",
       " 'Region_Baltics',\n",
       " 'full-stack_Yes',\n",
       " 'UndergradMajor_Human/Social Sci',\n",
       " 'EdLevel_None',\n",
       " 'JavaScript_Yes',\n",
       " 'Gender_gender non-conforming']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats_remove = remove_features(X_train_smoted, y_train_smoted, X_val_smoted, y_val_smoted, basef1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/racheldilley/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:4305: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "X_train_adasyn.drop(labels= feats_remove , axis=1, inplace=True)\n",
    "X_val_adasyn.drop(labels= feats_remove , axis=1, inplace=True)\n",
    "X_test_adasyn.drop(labels= feats_remove , axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost with params:\n",
      "max_depth = 30\n",
      "learning_rate = 0.3\n",
      "subsample = 0.8\n",
      "min_child_weight = 3\n",
      "colsample_bytree = 0.8\n",
      "\n",
      "Has an f1 score of: 0.854\n"
     ]
    }
   ],
   "source": [
    "#increase max_depth\n",
    "xgb2_smoted = XGBoost(X_train_smoted, y_train_smoted ,X_val_smoted, y_val_smoted, 30, 0.3, 0.8, 3, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost with params:\n",
      "max_depth = 100\n",
      "learning_rate = 0.3\n",
      "subsample = 0.8\n",
      "min_child_weight = 3\n",
      "colsample_bytree = 0.8\n",
      "\n",
      "Has an f1 score of: 0.853\n"
     ]
    }
   ],
   "source": [
    "#increase max_depth more\n",
    "xgb3_smoted = XGBoost(X_train_smoted, y_train_smoted ,X_val_smoted, y_val_smoted, 100, 0.3, 0.8, 3, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost with params:\n",
      "max_depth = 20\n",
      "learning_rate = 0.3\n",
      "subsample = 0.8\n",
      "min_child_weight = 3\n",
      "colsample_bytree = 0.8\n",
      "\n",
      "Has an f1 score of: 0.854\n"
     ]
    }
   ],
   "source": [
    "#decrease max depth & increase n_estimators\n",
    "xgb4_smoted = XGBoost(X_train_smoted, y_train_smoted ,X_val_smoted, y_val_smoted, 20, 0.3, 0.8, 3, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost with params:\n",
      "max_depth = 20\n",
      "learning_rate = 0.05\n",
      "subsample = 0.8\n",
      "min_child_weight = 3\n",
      "colsample_bytree = 0.8\n",
      "\n",
      "Has an f1 score of: 0.857\n"
     ]
    }
   ],
   "source": [
    "#decrease learning rate\n",
    "xgb5_smoted = XGBoost(X_train_smoted, y_train_smoted ,X_val_smoted, y_val_smoted, 20, 0.05, 0.8, 3, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decrease subsample\n",
    "xgb6_smoted = XGBoost(X_train_smoted, y_train_smoted ,X_val_smoted, y_val_smoted, 20, 0.05, 0.7, 3, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost with params:\n",
      "max_depth = 10\n",
      "learning_rate = 0.1\n",
      "subsample = 0.8\n",
      "min_child_weight = 5\n",
      "colsample_bytree = 0.8\n",
      "\n",
      "Has an f1 score of: 0.847\n"
     ]
    }
   ],
   "source": [
    "xgb7_adasyn = XGBoost(X_train_adasyn, y_train_adasyn,X_val_adasyn, y_val_adasyn, 10, 0.1, 0.8, 5, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose xgb_adasyn with an f1 score of: 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Models/xgb_balanced.pkl\", \"wb\") as f:\n",
    "    pkl.dump(xgb2_adasyn, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['database_count', 'Age1stCode', 'YearsCodePro', 'Age', 'YearsCode',\n",
       "       'EdLevel_BA/BS', 'EdLevel_MA/PhD', 'EdLevel_Prof', 'EdLevel_Some Univ',\n",
       "       'EdLevel_Student', 'Gender_Woman', 'UndergradMajor_Comp Sci/Eng',\n",
       "       'UndergradMajor_Eng', 'UndergradMajor_Health/Nat Sci',\n",
       "       'UndergradMajor_Info Tech/Sys', 'UndergradMajor_Math/Stats',\n",
       "       'UndergradMajor_None', 'UndergradMajor_Web Dev', 'Region_Asia',\n",
       "       'Region_Australia', 'Region_Europe', 'Region_M East',\n",
       "       'Region_N America', 'Region_S America', 'Region_other', 'back-end_Yes',\n",
       "       'front-end_Yes', 'desktop_Yes', 'mobile_Yes', 'DevOps_Yes',\n",
       "       'Database admin_Yes', 'System admin_Yes', 'Student_Yes', 'Python_Yes',\n",
       "       'SQL_Yes', 'Java_Yes', 'HTML/CSS_Yes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_adasyn.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
