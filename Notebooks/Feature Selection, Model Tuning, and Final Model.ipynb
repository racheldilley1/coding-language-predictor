{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from scipy import stats\n",
    "\n",
    "#modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import ADASYN, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from classification_functions import  conf_matrix, plot_roc\n",
    "from sklearn.metrics import f1_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGBoost(X_train, y_train, X_val, y_val, depth, l_rate, subsample,  min_weight, col_sample):\n",
    "              \n",
    "    # rand_param = {\n",
    "    #                 'n_estimators': [30000], \n",
    "    #                 'max_depth': [3,7],\n",
    "    #                 'objective': [\"reg:squarederror\"],\n",
    "    #                 'learning_rate': [0.05, .2], \n",
    "    #                 'subsample': [0.5, 0.8],\n",
    "    #                 'min_child_weight': [1, 8],\n",
    "    #                 'colsample_bytree': [0.5, 0.8]\n",
    "    #              }\n",
    "\n",
    "    params = { \n",
    "                'n_estimators': 10000,\n",
    "                'max_depth': depth,\n",
    "                'objective':'multi:softprob',\n",
    "                'num_classes' :3,  \n",
    "                #'use_label_encoder':False,\n",
    "                'learning_rate': l_rate, \n",
    "                'subsample': subsample,\n",
    "                'min_child_weight':min_weight,\n",
    "                'colsample_bytree':col_sample,\n",
    "                'random_state' : 0,\n",
    "                'verbosity' : 0,\n",
    "                'n_jobs' : -1}\n",
    "\n",
    "    gbm = XGBClassifier()\n",
    "    gbm.set_params(**params)\n",
    "    gbm.fit(X_train, y_train)\n",
    "    preds = gbm.predict(X_val)\n",
    "    \n",
    "    print(f'XGBoost with params:\\n'\n",
    "          f'max_depth = {depth}\\n'\n",
    "          f'learning_rate = {l_rate}\\n'\n",
    "          f'subsample = {subsample}\\n'\n",
    "          f'min_child_weight = {min_weight}\\n'\n",
    "          f'colsample_bytree = {col_sample}\\n'\n",
    "          '\\n'\n",
    "          f'Has an f1 score of: {round(f1_score( y_val, preds, average=\"macro\"), 3)}'\n",
    "         )\n",
    "          \n",
    "    return gbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../Data/survey_data_cleaned2.pkl')\n",
    "#df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['database_count', 'Age1stCode', 'YearsCodePro', 'Age','YearsCode', 'EdLevel', \n",
    "           'Ethnicity', 'Gender', 'UndergradMajor', 'Region', \n",
    "        'Hobbyist', 'back-end', 'full-stack', 'front-end', 'desktop', 'mobile', 'DevOps', 'Database admin', \n",
    "        'Designer','System admin', 'Student', 'Other Occupation', 'Retired Dev','Sometimes Code at Work', \n",
    "       'JavaScript', 'Python', 'SQL', 'Java', 'HTML/CSS']]\n",
    "y = df['OpSys']\n",
    "\n",
    "X = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try ADASYN Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/racheldilley/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost with params:\n",
      "max_depth = 10\n",
      "learning_rate = 0.3\n",
      "subsample = 0.8\n",
      "min_child_weight = 3\n",
      "colsample_bytree = 0.8\n",
      "\n",
      "Has an f1 score of: 0.869\n"
     ]
    }
   ],
   "source": [
    "X_adasyn, y_adasyn = ADASYN(random_state=42).fit_sample(X,y)\n",
    "\n",
    "X_train_adasyn, X_test_adasyn, y_train_adasyn, y_test_adasyn = train_test_split(X_adasyn,y_adasyn, test_size=0.2, random_state=42)\n",
    "X_train_adasyn, X_val_adasyn, y_train_adasyn, y_val_adasyn = train_test_split(X_train_adasyn,y_train_adasyn, test_size=0.25, random_state=42)\n",
    "\n",
    "xgb_adasyn = XGBoost(X_train_adasyn, y_train_adasyn,X_val_adasyn, y_val_adasyn, 10, 0.3, 0.8, 3, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try SMOTE Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/racheldilley/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "X_smoted, y_smoted = SMOTE(random_state=42).fit_sample(X,y)\n",
    "\n",
    "X_train_smoted, X_test_smoted, y_train_smoted, y_test_smoted = train_test_split(X_smoted,y_smoted, test_size=0.2, random_state=42)\n",
    "X_train_smoted, X_val_smoted, y_train_smoted, y_val_smoted = train_test_split(X_train_smoted, y_train_smoted, test_size=0.25, random_state=42)\n",
    "\n",
    "xgb_smoted = XGBoost(X_train_smoted, y_train_smoted, X_val_smoted, y_val_smoted, 10, 0.3, 0.8, 3, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "XGBoost() missing 7 required positional arguments: 'X_val', 'y_val', 'depth', 'l_rate', 'subsample', 'min_weight', and 'col_sample'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-16d37aeaa9df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_train_under\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_under\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_under\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_under\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_under\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_under\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mxgb_under\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBoost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_under\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_under\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: XGBoost() missing 7 required positional arguments: 'X_val', 'y_val', 'depth', 'l_rate', 'subsample', 'min_weight', and 'col_sample'"
     ]
    }
   ],
   "source": [
    "X_under, y_under = RandomUnderSampler(random_state=42).fit_sample(X,y)\n",
    "X_train_under, X_test_under, y_train_under, y_test_under = train_test_split(X_under,y_under, test_size=0.2, random_state=42)\n",
    "X_train_under, X_val_under, y_train_under, y_val_under = train_test_split(X_train_under,y_train_under, test_size=0.25, random_state=42)\n",
    "\n",
    "xgb_under = XGBoost(X_train_under, y_train_under, X_val_under, y_val_under, 10, 0.3, 0.8, 3, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE performed the best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_tree(xgb_smoted, num_trees=0)\n",
    "plt.rcParams['figure.figsize'] = [50, 10]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_smoted.plot_importance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Models/xgb_balanced.pkl\", \"wb\") as f:\n",
    "    pkl.dump(xgb_smoted, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
